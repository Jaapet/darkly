je lance dirsearch pour qu'il teste tous les chemins possibles : python3 dirsearch.py -u "http://192.168.56.101/" 
on voit un [12:32:35] 200 -    53B - /robots.txt
c'est bizarre

curl http://192.168.56.101/robots.txt

on voit
User-agent: *
Disallow: /whatever
Disallow: /.hidden

il y a pleins de readme a telecharger, j'ai fait : 
cd ~
mkdir darkly_hidden
cd darkly_hidden

wget -r -np -nH --cut-dirs=1 -R "index.html*" \
http://192.168.56.101/.hidden/

mais ca ne fonctionne pas, wget respecte robots.txt et ne va pas plus loin.
wget lit http://site/robots.txt puis il applique ses règles (les lignes Disallow:) et évite de crawler certaines URLs dans ce cas, robots.txt dit :
Disallow: /.hidden
Donc wget se bloque lui-même et ne télécharge pas /.hidden/...

Avec robots=off, je dis à wget :
“ignore robots.txt”
“continue la récursion même si c'est disallowed” :

rm -rf ~/darkly_hidden
mkdir ~/darkly_hidden
cd ~/darkly_hidden
wget -r -np -nH --cut-dirs=1 -e robots=off \
"http://192.168.56.101/.hidden/"


la ca telecharge tous les fichiers, plus qu'a faire : grep -R "flag" .
le readme avec le flag est a l'adresse : http://192.168.56.101/.hidden/whtccjokayshttvxycsvykxcfm/igeemtxnvexvxezqwntmzjltkt/lmpanswobhwcozdqixbowvbrhw/